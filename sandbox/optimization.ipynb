{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 11:32:24.353655: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-07 11:32:24.353723: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-07 11:32:24.355207: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-07 11:32:24.364866: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-07 11:32:26.015873: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/faruk/anaconda3/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "import preprocessing\n",
    "import data_augmentation_1D as data_aug\n",
    "import optuna \n",
    "import tensorflow as tf\n",
    "import keras \n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 11:27:13.795720: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 11:27:14.177622: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 11:27:14.177893: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 11:27:14.182782: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 11:27:14.182890: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 11:27:14.182949: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 11:27:14.367133: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 11:27:14.367266: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 11:27:14.367277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-03-07 11:27:14.367330: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-07 11:27:14.367354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4701 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# Définir le chemin racine du dataset\n",
    "dataset_root = \"/mnt/c/Users/onerf/Documents/UMons/Master2/RHEA/src/db/\"\n",
    "\n",
    "# Charger les profils depuis les dossiers spécifiés dans le dataset\n",
    "profiles  = preprocessing.sleepers_dataset_from_directory(dataset_root + \"sleepers_db/\")\n",
    "profiles_test = preprocessing.sleepers_dataset_from_directory(dataset_root + \"test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape : (2220, 437, 1)\n"
     ]
    }
   ],
   "source": [
    "CLASSES = 3\n",
    "INPUT_SHAPE = (437,1)\n",
    "EPOCHS = 1\n",
    "N_TRAIN_EXAMPLES = 14208\n",
    "N_VALID_EXAMPLES = 400\n",
    "\n",
    "\n",
    "x, y = profiles.data, profiles.target\n",
    "# Redimensionnement des données pour ajouter une dimension supplémentaire,\n",
    "x = x.reshape(x.shape[0], x.shape[1], 1)\n",
    "print(f'x shape : {x.shape}')\n",
    "\n",
    "x_test, y_test = profiles_test.data, profiles_test.target\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n",
    "\n",
    "x, y = data_augmentation_1D.augment(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 11:29:09.680214: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 993423360 exceeds 10% of free system memory.\n",
      "2024-03-07 11:29:12.012532: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 993423360 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DatasetV2.shuffle() missing 1 required positional argument: 'buffer_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m32\u001b[39m)\u001b[38;5;241m.\u001b[39mtake(N_TRAIN_EXAMPLES)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# valid_ds = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# valid_ds = valid_ds.shuffle(len(x_test)).batch(32).take(N_VALID_EXAMPLES)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: DatasetV2.shuffle() missing 1 required positional argument: 'buffer_size'"
     ]
    }
   ],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x,y)).shuffle().batch(32).take(N_TRAIN_EXAMPLES)\n",
    "\n",
    "# valid_ds = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n",
    "# valid_ds = valid_ds.shuffle(len(x_test)).batch(32).take(N_VALID_EXAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(trial,n_layers,max_pooling =False, drop_out=False):\n",
    "    # We optimize the numbers of layers, their units and weight decay parameter.\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=INPUT_SHAPE))\n",
    "    model.add(\n",
    "        tf.keras.layers.BatchNormalization()\n",
    "        )\n",
    "    for i in range(n_layers):\n",
    "        num_hidden = trial.suggest_categorical(\"n_units_l{}\".format(i+1), [16, 32, 64, 128])\n",
    "\n",
    "        kernel_size = trial.suggest_int(\"kernel_size\", 1, 3)\n",
    "\n",
    "        model.add(\n",
    "            tf.keras.layers.Conv1D(num_hidden,kernel_size=kernel_size,padding=\"SAME\",activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "            )\n",
    "        model.add(\n",
    "          tf.keras.layers.BatchNormalization()\n",
    "        )\n",
    "        if(max_pooling):\n",
    "          max_pool = trial.suggest_int(\"max_pool\", 2, 3)\n",
    "          model.add(\n",
    "              tf.keras.layers.MaxPool1D(pool_size=max_pool)\n",
    "          )\n",
    "        if(drop_out):\n",
    "          drop_out = trial.suggest_float(\"drop_out\", 0.1, 0.5, log=True)\n",
    "          model.add(\n",
    "              tf.keras.layers.Dropout(drop_out)\n",
    "          )\n",
    "\n",
    "          \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(CLASSES, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(model, dataset, mode=\"eval\"):\n",
    "    accuracy = tf.metrics.Accuracy(\"accuracy\", dtype=tf.float32)\n",
    "\n",
    "    for batch, (images, labels) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(images, training=(mode == \"train\"))\n",
    "            loss_value = tf.reduce_mean(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "            )\n",
    "            if mode == \"eval\":\n",
    "                accuracy(\n",
    "                    tf.argmax(logits, axis=1, output_type=tf.int64), tf.cast(labels, tf.int64)\n",
    "                )\n",
    "            else:\n",
    "                grads = tape.gradient(loss_value, model.variables)\n",
    "                optimizer.apply_gradients(zip(grads, model.variables))\n",
    "\n",
    "    if mode == \"eval\":\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    max_pooling = np.random.choice([True, False])\n",
    "    drop_out = np.random.choice([True, False])\n",
    "    model = create_model(trial,2,max_pooling,drop_out)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "   \n",
    "    # Training and validating cycle.\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "      for _ in range(EPOCHS):\n",
    "        model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                            loss='sparse_categorical_crossentropy',\n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "    # Return last validation accuracy.\n",
    "    return model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
